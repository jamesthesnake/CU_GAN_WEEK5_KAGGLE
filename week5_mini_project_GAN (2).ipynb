{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooyU1Blk5Y0T"
      },
      "source": [
        "# Use GANs to create art Mini-Project\n",
        "\n",
        "* Kaggle Competition [I’m Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started/overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eczk9M_Q5Y0U",
        "outputId": "7d0df3ae-3044-4561-f644-6149e4e744c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "# Packages\n",
        "!pip install tensorflow_addons\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Tensorflow config\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "546OvVJF5Y0V",
        "outputId": "8e6e4417-b7ef-4b1d-a530-642390d31832"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Check if GPU is used\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hwlVU8O5Y0V"
      },
      "source": [
        "The code below sets the model training strategy according to the processor type, and outputs the processor details. Kaggle's GPU T4 X2 and TPU VM v3-8 were not compatible with TensorFlow due to incompatibilities with CUDA version 12.2 and possibly with cuDNN. Using Kaggle's GPU P100 returned warning messages that hinted at compatibility problems, but the model compiled and trained without issue.\n",
        "\n",
        "[Ref](https://www.kaggle.com/code/stevenhobbs74/hobbs-monet-cyclegan#INTRODUCTION-&-SETUP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvpQm9sF5Y0V",
        "outputId": "86ca7c5c-20f1-4f18-df5b-a99ad947cca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU not available.\n",
            "Running on CPU.\n",
            "Number of accelerators (CPU, GPU, or TPU): 1\n",
            "Tensorflow Version: 2.15.0\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime.\n"
          ]
        }
      ],
      "source": [
        "# Kaggle notebook setting\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "    tf.config.experimental_connect_to_cluster(tpu) # Connects to the given cluster\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu) # Initializes the TPU system\n",
        "    strategy = tf.distribute.TPUStrategy(tpu) # TPU distribution strategy\n",
        "    print('Running on TPUs')\n",
        "\n",
        "except ValueError:\n",
        "    print(\"TPU not available.\")\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "        print(\"Running on GPU.\")\n",
        "    else:\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        print(\"Running on CPU.\")\n",
        "\n",
        "print('Number of accelerators (CPU, GPU, or TPU):', strategy.num_replicas_in_sync)\n",
        "print(\"Tensorflow Version:\", tf.__version__)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "    print('Not using a high-RAM runtime.')\n",
        "else:\n",
        "    print('Using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ai-jjGH5Y0V"
      },
      "source": [
        "Goal:\n",
        "\n",
        "The primary objective of this project is to train a Generative Adversarial Network (GAN) capable of generating images in the style of Monet. This involves the development and training of at least two neural network models: a generator and a discriminator. The generator model is tasked with creating images resembling Monet paintings, while the discriminator model aims to distinguish between real Monet paintings and generated images. A key highlight of this project is the exploration of two different architectures with distinct designs and hyperparameter settings.\n",
        "\n",
        "#Data and Project Description:\n",
        "\n",
        "#Dataset Composition:\n",
        "\n",
        "The training dataset consists of 300 Monet paintings and 7,028 photos for style transformation. All images are sized 256x256 pixels and are available in both JPEG and TFRecord formats.\n",
        "CycleGAN:\n",
        "\n",
        "CycleGAN is the chosen model for this project, primarily utilized for solving the image-to-image translation problem. Unlike traditional methods that require paired input-output images for training, CycleGAN can learn the mapping between input and output images without such pairs. This is particularly advantageous given the limited availability of paired examples in our dataset.\n",
        "TFRecord Format:\n",
        "\n",
        "TFRecord is TensorFlow's proprietary binary record format. It offers several advantages, including reduced disk space usage and faster read/write times compared to traditional formats. TFRecord also facilitates the integration of multiple datasets into a single dataset, streamlining data loading and preprocessing operations. This is especially beneficial when dealing with large datasets, as it allows for efficient memory usage by loading only the necessary portions of the TFRecord for processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sUh8IiT5Y0W"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w7Y-qoj5Y0W"
      },
      "source": [
        "A. Load and Data Preprocessing\n",
        "In this section, we will utilize Tensorflow's TFRecord files and employ the tf.data.Dataset API to load and parse the data. The data input pipeline will encompass the following tasks:\n",
        "\n",
        "Parsing the Data: This involves converting the serialized TFRecords into TensorFlow dictionaries.\n",
        "Decoding the TensorFlow String: Decode the TensorFlow string into a 3D tensor.\n",
        "Setting Data Type: Set the data type to float32.\n",
        "Scaling the Data: Scale the data to a range of [-1, 1].\n",
        "Reshaping the Data: Reshape the data to dimensions of 256 x 256.\n",
        "These preprocessing steps ensure that the data is properly formatted and prepared for training the GAN model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1YzU9Pj5Y0W",
        "outputId": "fa3b96e5-80c6-4f56-be7a-fce80723c7ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monet TFRecord Files: 0\n",
            "Photo TFRecord Files: 0\n"
          ]
        }
      ],
      "source": [
        "image = Path.home() /\"Desktop/gan-getting-started\"\n",
        "MONET = tf.io.gfile.glob(str(image.as_posix() + '/monet_tfrec/*.tfrec'))\n",
        "print('Monet TFRecord Files:', len(MONET))\n",
        "\n",
        "PHOTO = tf.io.gfile.glob(str(image.as_posix() + '/photo_tfrec/*.tfrec'))\n",
        "print('Photo TFRecord Files:', len(PHOTO))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qTRs-k3p5Y0W"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "image_size = [256, 256]\n",
        "\n",
        "def image_decoder(image_input):\n",
        "    image = tf.image.decode_jpeg(image_input, channels=3)\n",
        "    # Scale the images to [-1, 1]\n",
        "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
        "    image = tf.reshape(image, [*image_size, 3])\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    # Decode the TFRecord\n",
        "    tfrecord_format = {\n",
        "        \"image_name\" : tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image\" : tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\" : tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = image_decoder(example['image'])\n",
        "    return image\n",
        "\n",
        "# Extract the image from files\n",
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1abwagKK5Y0W"
      },
      "outputs": [],
      "source": [
        "# Load images\n",
        "monet_load = load_dataset(MONET, labeled=True)\n",
        "photo_load = load_dataset(PHOTO, labeled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fejDZ5om5Y0W",
        "outputId": "5d6794b0-5cc1-4813-8cdc-ae0c7dcd6e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of dataset sequence reached.\n",
            "End of dataset sequence reached.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    example_monet = next(iter(monet_load))\n",
        "except StopIteration:\n",
        "    print(\"End of dataset sequence reached.\")\n",
        "\n",
        "try:\n",
        "    example_photo = next(iter(photo_load))\n",
        "except StopIteration:\n",
        "    print(\"End of dataset sequence reached.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "er-PF1li5Y0X",
        "outputId": "47788074-1a63-49bc-d87b-fdbacad51a9d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'example_photo' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6d80d5a1b9e8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Photorealstic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_photo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'example_photo' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Photorealstic')\n",
        "plt.imshow(example_photo[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Monet photos')\n",
        "plt.imshow(example_monet[0] * 0.5 + 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkVnIcYq5Y0X"
      },
      "source": [
        "### B. Data augmentation\n",
        "\n",
        "* For tasks like transferring the style of one image to another, it's crucial to approach data augmentation with caution. Drastically altering colors, such as brightness, contrast, or saturation, may confuse the machine and hinder its ability to learn the original style effectively. Therefore, it's advisable to limit color modifications and instead focus on non-color-related transformations like flipping, rotating, or cropping the images. This approach allows the machine to concentrate on learning the fundamental style nuances present in the original data. Additionally, introducing some randomness during augmentation helps introduce diversity and mitigate the risk of overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZLmwtpv5Y0X"
      },
      "outputs": [],
      "source": [
        "# Data augmentation for the images\n",
        "def aug_data(image_input):\n",
        "    # Use random choices for augmentations to introduce variety and prevent overfitting.\n",
        "    spatial_r = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "    rotate_r = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n",
        "\n",
        "    # Apply 90 degrees rotations\n",
        "    if rotate_r > .8:\n",
        "        image_input = tf.image.rot90(image_input, k=3)  # Rotate 270º\n",
        "    elif rotate_r > .6:\n",
        "        image_input = tf.image.rot90(image_input, k=2)  # Rotate 180º\n",
        "    elif rotate_r > .4:\n",
        "        image_input = tf.image.rot90(image_input, k=1)  # Rotate 90º\n",
        "\n",
        "    # Apply flips\n",
        "    image_input = tf.image.random_flip_left_right(image_input)\n",
        "    image_input = tf.image.random_flip_up_down(image_input)\n",
        "\n",
        "    # Transpose operation\n",
        "    if spatial_r > .75:\n",
        "        image_input = tf.image.transpose(image_input)\n",
        "\n",
        "    # Focus on a 128x128 pixel area to promote learning from different image details.\n",
        "    image_input = tf.image.random_crop(image_input, size=[128, 128, 3])\n",
        "\n",
        "    return image_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sccheX385Y0X"
      },
      "outputs": [],
      "source": [
        "monet_dataset = monet_load.map(aug_data, num_parallel_calls=AUTOTUNE)\n",
        "photo_dataset = photo_load.map(aug_data, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "monet_dataset = monet_dataset.batch(1).shuffle(1024).prefetch(AUTOTUNE)\n",
        "photo_dataset = photo_dataset.batch(1).shuffle(1024).prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AqsrUxS5Y0X"
      },
      "source": [
        "## Structure Design\n",
        "\n",
        "* What is U-Net?\n",
        "The CycleGAN architecture incorporates a U-Net structure, which is renowned for its effectiveness in image segmentation tasks. Before delving into the specifics of the U-Net implementation, let's first understand the key principles behind the U-Net architecture.\n",
        "\n",
        "#### 1: Capturing Details (Contracting Path):\n",
        "\n",
        "  * Similar to a detective gathering clues, U-Net begins its analysis by scrutinizing the image in detail. It employs a series of convolutional layers, acting as magnifying glasses, to zoom in and capture crucial features such as shapes, edges, and textures. This initial phase of U-Net, known as the contracting path, progressively extracts essential details from the image while maintaining their integrity as it delves deeper.\n",
        "\n",
        "#### 2  Locating Precise Areas (Expanding Path):\n",
        "\n",
        "* Having gained a comprehensive understanding of the image's intricacies, U-Net proceeds to pinpoint specific regions of interest. Here, U-Net's second superpower, the expanding path, comes into play. Through specialized techniques, it enhances the resolution of extracted information and merges it with the detailed features obtained earlier. This process resembles overlaying precise location markings onto a detailed map, enabling U-Net to accurately identify and segment areas of interest within the image.\n",
        "\n",
        "###Summary:\n",
        "\n",
        "During training, U-Net undergoes rigorous challenges. It is presented with real images containing segmented areas and tasked with predicting these segmentations independently. In the event of errors, U-Net receives corrective feedback, nudging it towards improved performance. This competitive interaction between U-Net and the training data fosters its growth into a proficient segmentation expert.\n",
        "\n",
        "In summary, U-Net's amalgamation of detailed feature extraction and precise localization renders it an invaluable tool for addressing image segmentation tasks with remarkable accuracy and efficiency.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Implemented Structure\n",
        "\n",
        "In constructing the unet-style generator, we'll begin by defining functions for downsampling and upsampling the input images. This approach is illustrated in both the Tensorflow CycleGAN Tutorial and Amy Jang's CycleGAN tutorial, both of which offer valuable insights. Some of the code presented here is adapted from these tutorials.\n",
        "\n",
        "## Downsampling:\n",
        "Downsampling involves reducing the image's two-dimensional size (width and height) by a factor determined by the stride. The stride specifies the distance the filter moves between applications. With a stride set to 2, the filter is applied to every other pixel, effectively halving both the width and height.\n",
        "\n",
        "## Upsampling:\n",
        "Conversely, upsampling increases the dimensions of the image, providing a higher resolution output.\n",
        "\n",
        "* Both tutorials opted for instance normalization from Tensorflow Addons (tfa.layers.InstanceNormalization) instead of the more common batch normalization (tf.keras.layers.BatchNormalization). Since Tensorflow no longer natively supports instance normalization, we'll utilize group normalization (tensorflow.keras.layers.GroupNormalization). To achieve the same effect as InstanceNormalization, we'll set groups = -1 or groups equal to the number of input channels. For more information about GroupNormalization, please refer to the Keras V2 document and Keras V3 document.\n",
        "\n",
        "The downsample and upsample functions combine standard layers (Conv2D/Conv2DTranspose, GroupNormalization, Dropout, and LeakyReLU/ReLU) into blocks, which are concise and facilitate easier manipulation, while still allowing specification of filter numbers and kernel sizes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfpOZxtl5Y0X"
      },
      "outputs": [],
      "source": [
        "CHANNELS = 3\n",
        "\n",
        "def downsample_func(filters, kernel_size=4, apply_instancenorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2D(\n",
        "        filters, kernel_size,\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        kernel_initializer=initializer,\n",
        "        use_bias=False))\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        result.add(layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init))\n",
        "\n",
        "    result.add(layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def upsample_func(filters, kernel_size=4, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    return_result = keras.Sequential()\n",
        "    return_result.add(layers.Conv2DTranspose(\n",
        "        filters, kernel_size,\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        kernel_initializer=initializer,\n",
        "        use_bias=False))\n",
        "\n",
        "    return_result.add(layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init))\n",
        "\n",
        "    if apply_dropout:\n",
        "        return_result.add(layers.Dropout(0.5))\n",
        "\n",
        "    return_result.add(layers.ReLU())\n",
        "\n",
        "    return return_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eamw0wo65Y0X"
      },
      "source": [
        "### A. Build\n",
        "\n",
        "The generator is constructed using a combination of different layers along with skip connections:\n",
        "\n",
        "## Convolutional Layers (Downsample) or Transpose Convolutional Layers (Upsample):\n",
        "  * These layers either extract features from the image (downsampling) or inject features into the image (upsampling), aiding in tasks such as identifying edges or adding details.\n",
        "\n",
        "## Group Normalization:\n",
        "  * Incorporated to enhance the learning process of the network.\n",
        "\n",
        "## Activation Function (ReLU or LeakyReLU):\n",
        "   * Adds non-linearity to the process, enabling the network to learn complex patterns effectively.\n",
        "   \n",
        "## Dropout Layer:\n",
        "   * This layer randomly drops some information during training to prevent overfitting, thereby making the model more adaptable to varying scenarios.\n",
        "\n",
        "\n",
        "The final layer utilizes a different activation function (tanh) to produce the generated image with values in the range of  -1 and 1.\n",
        "\n",
        "#Shortcut Connections (Skip Connections):\n",
        "\n",
        " * Skip connections serve as useful reminders of important details from earlier stages. They establish direct connections between information from earlier steps in the generator to later stages, aiding in retaining crucial details about the original image and preventing loss of information across multiple steps. These connections are depicted as curved arrows in the diagram following the function definitions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bi1F4mQ5Y0Y"
      },
      "outputs": [],
      "source": [
        "# Build the generator\n",
        "def Generator():\n",
        "    inputs = layers.Input(shape=[256, 256, 3])\n",
        "    up_stack = [\n",
        "        upsample_func(512, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
        "        upsample_func(512, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
        "        upsample_func(512, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
        "        upsample_func(512),  # (bs, 16, 16, 1024)\n",
        "        upsample_func(256),  # (bs, 32, 32, 512)\n",
        "        upsample_func(128),  # (bs, 64, 64, 256)\n",
        "        upsample_func(64),  # (bs, 128, 128, 128)\n",
        "    ]\n",
        "    down_stack = [\n",
        "        downsample_func(64, apply_instancenorm=False),  # (bs, 128, 128, 64), bs = batch size\n",
        "        downsample_func(128),  # (bs, 64, 64, 128)\n",
        "        downsample_func(256),  # (bs, 32, 32, 256)\n",
        "        downsample_func(512),  # (bs, 16, 16, 512)\n",
        "        downsample_func(512),  # (bs, 8, 8, 512)\n",
        "        downsample_func(512),  # (bs, 4, 4, 512)\n",
        "        downsample_func(512),  # (bs, 2, 2, 512)\n",
        "        downsample_func(512),  # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(3, 4,\n",
        "                                   strides=2,\n",
        "                                   padding='same',\n",
        "                                   kernel_initializer=initializer,\n",
        "                                   activation='tanh')  # (bs, 256, 256, 3)\n",
        "\n",
        "    index = inputs\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = []\n",
        "    for down_unit in down_stack:\n",
        "        index = down_unit(index)\n",
        "        skips.append(index)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        index = up(index)\n",
        "        index = layers.Concatenate()([index, skip])\n",
        "\n",
        "    index = last(index)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3H1h5Z05Y0Y"
      },
      "source": [
        "# Discriminator\n",
        "The discriminator comprises three primary layers:\n",
        "\n",
        "###Downsampling Layers:\n",
        " * These layers utilize three \"downsample\" blocks to progressively extract more complex features from the input image. They start with low-level details like edges and colors and proceed to capture higher-level abstract features in deeper layers.\n",
        "\n",
        "###Bottleneck Layers:\n",
        "\n",
        "* Zero Padding Layer:\n",
        " ##### This layer adds padding to the feature maps generated by the preceding downsampling layer (typically referred to as 'down3'). The padding ensures that the output size remains unchanged after subsequent convolution operations.\n",
        "\n",
        "* Convolutional Layer\n",
        "\n",
        "* Group Normalization\n",
        "\n",
        "* LeakyReLU\n",
        "\n",
        " ####### This bottleneck layer, consisting of convolution, normalization, and activation, serves as the discriminator's core. It is responsible for extracting high-level features crucial for discerning between real and fake images.\n",
        "\n",
        "### Output Layer:\n",
        "\n",
        "* Zero Padding Layer\n",
        "* Convolutional Layer\n",
        "#### The output layer produces a smaller 2D image where higher pixel values indicate a classification as real, while lower values suggest a classification as fake.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC6t6peD5Y0Y"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    # Input layer\n",
        "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "    x = inp\n",
        "\n",
        "    # Downsampling layers\n",
        "    down1 = downsample_func(64, apply_instancenorm=False)(x)  # (bs, 128, 128, 64)\n",
        "    down2 = downsample_func(128)(down1)  # (bs, 64, 64, 128)\n",
        "    down3 = downsample_func(256)(down2)  # (bs, 32, 32, 256)\n",
        "\n",
        "    # Bottleneck layers\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
        "    conv = layers.Conv2D(\n",
        "        512, 4,\n",
        "        strides=1,\n",
        "        kernel_initializer=initializer,\n",
        "        use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
        "\n",
        "    norm1 = tfa.layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init)(conv)\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "\n",
        "    # Output layer\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
        "    fin_output = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=fin_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agjDAdv35Y0Y"
      },
      "source": [
        "### C. Architecture diagram & model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioYCL8r95Y0Y"
      },
      "outputs": [],
      "source": [
        "# strategy = tf.distribute.get_strategy()\n",
        "# APPLY DISTRIBUTION STRATEGY TO MODELS\n",
        "with strategy.scope():\n",
        "    monet_gen = Generator() # transforms photos to Monet-esque paintings\n",
        "    photo_gen = Generator() # transforms Monet paintings to be more like photos\n",
        "\n",
        "    monet_disc = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n",
        "    photo_disc = Discriminator() # differentiates real photos and generated photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaYbXBh_5Y0Y"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECT5Pwrx5Y0Y"
      },
      "outputs": [],
      "source": [
        "print(\"Monet Generator Model Summary:\", monet_gen.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n68HzP8Z5Y0Y"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ff7PMr3F_rd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EjO1dR_h_raB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8gvWr1x5Y0Y"
      },
      "outputs": [],
      "source": [
        "print(\"Monet Discriminator Model Summary:\", monet_disc.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mp3MKC-5Y0Y"
      },
      "outputs": [],
      "source": [
        "to_monet = monet_gen(example_photo)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original Photo\")\n",
        "plt.imshow(example_photo[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Monet-esque Photo\")\n",
        "plt.imshow(to_monet[0] * 0.5 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtbHQVNJ5Y0Z"
      },
      "source": [
        "###  CycleGAN model\n",
        "\n",
        "The CycleGAN architecture is structured around a subclass of tf.keras.Model, facilitating training via the fit() method for image translation between Monet paintings and photos. It comprises two key components:\n",
        "\n",
        "####  **Generators**:\n",
        " * Two generators (m_gen and p_gen) are responsible for translating images between Monet paintings and photos.\n",
        "\n",
        "####  **Discriminators**:\n",
        "\n",
        "  * Two discriminators (m_disc and p_disc) distinguish between real Monet paintings/photos and generated ones.\n",
        "\n",
        "The training process involves several steps:\n",
        "\n",
        "1. **Generator Training**: Fake images are generated (Monet paintings from photos and photos from Monet paintings).\n",
        "\n",
        "2. **Cycle Consistency**: The generated images are cycled back to their original domain (photo -> fake Monet -> photo or Monet -> fake photo -> Monet), ensuring resemblance to the originals.\n",
        "\n",
        "3. **Identity Loss**: Generators are penalized for significantly altering real images during translation, aiming to preserve some identity.\n",
        "\n",
        "4. **Discriminator Training**: Discriminators aim to differentiate between real and fake images, while generators aim to fool them.\n",
        "\n",
        "The training loop iterates through the dataset, calculating losses for generators and discriminators based on the outlined strategies. Gradients are then computed and applied to update the weights of generators and discriminators, enhancing their performance over time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH6jBZgj5Y0Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        monet_gen,\n",
        "        photo_gen,\n",
        "        monet_disc,\n",
        "        photo_disc,\n",
        "        lambda_cycle=10,\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.m_gen = monet_gen\n",
        "        self.p_gen = photo_gen\n",
        "        self.m_disc = monet_disc\n",
        "        self.p_disc = photo_disc\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        m_gen_optimizer,\n",
        "        p_gen_optimizer,\n",
        "        m_disc_optimizer,\n",
        "        p_disc_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "        cycle_loss_fn,\n",
        "        identity_loss_fn\n",
        "    ):\n",
        "        super(CycleGan, self).compile()\n",
        "        self.m_gen_optimizer = m_gen_optimizer\n",
        "        self.p_gen_optimizer = p_gen_optimizer\n",
        "        self.m_disc_optimizer = m_disc_optimizer\n",
        "        self.p_disc_optimizer = p_disc_optimizer\n",
        "        self.gen_loss_fn = gen_loss_fn\n",
        "        self.disc_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = cycle_loss_fn\n",
        "        self.identity_loss_fn = identity_loss_fn\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        real_monet, real_photo = batch_data\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            fake_monet = self.m_gen(real_photo, training=True)\n",
        "            cycled_photo = self.p_gen(fake_monet, training=True)\n",
        "\n",
        "            fake_photo = self.p_gen(real_monet, training=True)\n",
        "            cycled_monet = self.m_gen(fake_photo, training=True)\n",
        "\n",
        "            same_monet = self.m_gen(real_monet, training=True)\n",
        "            same_photo = self.p_gen(real_photo, training=True)\n",
        "            disc_fake_monet = self.m_disc(fake_monet, training=True)\n",
        "            disc_fake_photo = self.p_disc(fake_photo, training=True)\n",
        "\n",
        "            disc_real_monet = self.m_disc(real_monet, training=True)\n",
        "            disc_real_photo = self.p_disc(real_photo, training=True)\n",
        "\n",
        "\n",
        "            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n",
        "            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n",
        "\n",
        "            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
        "\n",
        "            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n",
        "            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n",
        "\n",
        "            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n",
        "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n",
        "\n",
        "        monet_generator_gradients = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n",
        "        photo_generator_gradients = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n",
        "\n",
        "        monet_discriminator_gradients = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n",
        "        photo_discriminator_gradients = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n",
        "        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients, self.m_disc.trainable_variables))\n",
        "        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients, self.p_disc.trainable_variables))\n",
        "\n",
        "        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients, self.m_gen.trainable_variables))\n",
        "        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients, self.p_gen.trainable_variables))\n",
        "\n",
        "\n",
        "        return {\n",
        "              \"photo_disc_loss\": photo_disc_loss,\n",
        "             \"photo_gen_loss\": total_photo_gen_loss,\n",
        "\n",
        "\n",
        "            \"monet_gen_loss\": total_monet_gen_loss,\n",
        "            \"monet_disc_loss\": monet_disc_loss,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3tDL7Wd5Y0Z"
      },
      "source": [
        "###  Loss functions\n",
        "\n",
        "* The utilization of strategy.scope() in Tensorflow directs the distribution of the model's operations as per the strategy defined in the \"Kaggle notebook setting\" section following the package imports.\n",
        "\n",
        "* The significance of each loss function has been elucidated in the preceding section.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX6oia9S5Y0Z"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real, generated):\n",
        "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n",
        "    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "    return total_disc_loss * 0.5\n",
        "\n",
        "def generator_loss(generated):\n",
        "    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n",
        "\n",
        "def calc_cycle_loss(real_image, cycled_image, lambda_const):\n",
        "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "    return lambda_const * loss1\n",
        "\n",
        "def identity_loss(real_image, same_image, lambda_const):\n",
        "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "    return lambda_const * 0.5 * loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SEsLZHV5Y0Z"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhzYtKvq5Y0Z"
      },
      "outputs": [],
      "source": [
        "# Define optimizer\n",
        "with strategy.scope():\n",
        "    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43MHoLm15Y0Z"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "with strategy.scope():\n",
        "    cycle_gan_model = CycleGan(\n",
        "        monet_gen, photo_gen, monet_disc, photo_disc\n",
        "    )\n",
        "\n",
        "    cycle_gan_model.compile(\n",
        "        m_gen_optimizer = monet_generator_optimizer,\n",
        "        p_gen_optimizer = photo_generator_optimizer,\n",
        "        m_disc_optimizer = monet_discriminator_optimizer,\n",
        "        p_disc_optimizer = photo_discriminator_optimizer,\n",
        "        gen_loss_fn = generator_loss,\n",
        "        disc_loss_fn = discriminator_loss,\n",
        "        cycle_loss_fn = calc_cycle_loss,\n",
        "        identity_loss_fn = identity_loss\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48swuwHv5Y0Z"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "history = cycle_gan_model.fit(\n",
        "    tf.data.Dataset.zip((monet_dataset, photo_dataset)),\n",
        "    epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxEJADkN5Y0Z"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "##  submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQlGfUn55Y0Z"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "! mkdir ../images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIoAhOJs5Y0g"
      },
      "outputs": [],
      "source": [
        "i = 1\n",
        "for img in photo_dataset:\n",
        "    prediction = monet_gen(img, training=False)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    im = PIL.Image.fromarray(prediction)\n",
        "    im.save(\"../images/\" + str(i) + \".jpg\")\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6S6Dyji5Y0g"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuA14EW55Y0g"
      },
      "source": [
        "After several iterations, the initial notebook achieved a score of around 60. Subsequent attempts to improve the score through techniques such as data augmentation and increasing training epochs actually yielded worse results. It's evident that merely tweaking these parameters wasn't sufficient for enhancing performance. It's clear that adjusting the model architecture is necessary for further improvement.\n",
        "\n",
        "Throughout this project, various techniques have been applied to enhance the performance of CycleGAN:\n",
        "\n",
        "* Utilizing InstanceNormalization\n",
        "* Implementing a DCGAN structure\n",
        "* Employing a small batch size for training\n",
        "* Adopting the Adam optimizer\n",
        "* Integrating skip connections\n",
        "Due to constraints on GPU and TPU usage on Kaggle, further experimentation with hyperparameter tuning or alternative structures wasn't feasible. However, I intend to explore these avenues locally where resource limitations are less restrictive.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVimcZVw5Y0g"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}